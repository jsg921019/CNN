{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lenet-5 (1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 아키텍쳐의 할아버지급의 위상을 가지는 Lenet-5 모델에 대해 알아보고자 한다. 이 모델이 소개된 논문의 주요 내용은 잘 설계된 신경망에 대한 효율성에 대한 입증과 Graph Transformer Network으로 Lenet은 하나의 성공적인 예시로 어찌보면 조연처럼 나오지만, 그래도 우리가 관심있는 것은 Lenet모델이니 이 글에서는 Lenet에만 집중하록 하겠다.\n",
    "\n",
    "> 논문 : [GradientBased Learning Applied to Document Recognition [1998]](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅰ. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "better pattern recognition system can be built by relying more on automatic learning and less on hand-designed heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Lenet/traditional_pattern_recognition.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당시 패턴 인식 문제에서 가장 보편적인 방법은 사람이 직접 설계한 모듈을 통해 Feature vector를 추출하여 이를 입력으로 분류기를 훈련시키는 방식이였다.\n",
    "이러한 방식에는 크게 두가지의 문제점이 있었는데,  \n",
    "1. 모듈 설계 자체의 어려움과,\n",
    "2. 다른 task마다 매번 새로운 모듈을 설계해야 한다는 점이다.\n",
    "이러한 문제점에도 불구하고 이러한 방식을 썼던 이유는 분류기가 낮은 차원의 입력값에서만 성능을 보였기 때문이다.\n",
    "\n",
    "하지만 다음과 같은 3가지의 변화로 상황이 바뀌었따\n",
    "1. 컴퓨터의 계산 성능의 향상\n",
    "2. 더 많은 수의 데이터를 사용할 수 있게 되었다.\n",
    "3. 머신러닝 기법들의 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅱ. Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extractor에 neural network를 사용하지 않았던 이유? Linear Layer의 경우 이미지의 input이 몇백에 달하기 때문에 매우 많은 수의 parameter가 필요하다. 이런 많은 parameter를 잘 학습시키기 위해서는 더 큰 training dataset이 필요하다. 또한 하드웨어의 메모리 용량에 제한에 걸릴 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 입문자라면 한번씩 써보게 되는 MNIST 데이터셋에 CNN 구조를 성공적으로 접목시킨 첫 모델이다. input data에서 사람이 직접 feature를 추출하여 모델에 넣기보다 raw 한 데이터를 넣었다. data에 대한 최소한의 지식이 있는 신경망을 사용해야 한다. 가장 중요한 문제점은 input의 translation이나 distortion에 취약할 수 있다는 점ㅁ이다.(학습이 될지언정 오래 걸릴 것이다) convolution network는 이러한 취약점을 태생적으로 극복할 수 있게 설계되었다.  \n",
    "두번째로 linear layer는 input의 위상적 특성을 완전히 무시한다. CNN은 receptive field에서만 connection 가지게 강제시킨다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer의 문제점 : parmeter가 너무 많다. - Generalize가 잘 안된다.\n",
    "CNN의 경우: local connection만 갖게 제한함, shared weight를 가짐 => feature map을 여러개 생성\n",
    "\n",
    "<img src=\"img/Lenet/convolution.png\" width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Layer는 shift, scale, distortion에 내성을 갖기 위해 크게 3가지의 발상에서 설계되었다.\n",
    "1. local receptive field\n",
    "2. shared weights\n",
    "3. sub-sampling\n",
    "이러한 개념을 적용하면 input의 모든 receptive field에 shared weight를 적용한 연산을 하면 feature map을 얻을 수 있다. 완전한 convolution layer는 각기 다른 weight로 여러 feature map을 추출한다.\n",
    "위의 예시에서 feature map의 하나의 unit은 input의 5x5 receptive field에서 25개의 weight(그리고 1개의 bias)로 얻은 연산의 결과인 것이다. 이를 한칸씩 이동시키면서 전체 feature map을 얻을 수 있다.(이 과정에서 전 영역과 겹치는 구간이 생긴다). 모든 영역에서 같은 weightd와 bias 사용하므로 모든 영역에서 같은 특징을 추출한다. 다른 weight를 사용하여 다른 특징을 담은 feature map을 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한번 특징을 추출하게 되면 그 위치적 정보의 중요성은 떨어진다. 예를 들어 왼쪽 위에서 꼭지점을 특징이 발견되었으면 0이 아닐것이라고 추측할 수 있다. 그 꼭지점의 정확한 위치는 크게 중요하지 않다.(오히려 해가 될 수 있다. 같은 숫자에서도 다르므로). feature map의 정보를 떨어트리는 방법으로 해상도를 낮추는 것이 있고 이는 sub-sampling layer로 이러한 목적을 달성할 수 있다. 이 layer는 영역의 모든 값들의 평균값을 구하여 weight를 곱하고 bias를 더한다음에 sigmoid function을 통과한다.(현재의 자주 쓰이는 parameter가 없는 pooling layer랑 다르다). 해상도를 낮추는 것이 목표이기 때문에 convolution layer와 달리 영역간 overlap이 일어나지 않고 결과적으로 너비와 높이가 1/2이 된다(2x2커널일시).\n",
    "통상적으로 전체 신경망은 convolution layer와 subsampling layer를 하나씩 반복하면서 이루어진다. 이러한 망을 통과하면서 spatial resolution이 낮은 많은 feature를 얻을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅲ. Lenet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Lenet/lenet.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenet-5는 총 7개의 trainable한 parameter를 가진 layer로 구성되어 있다.\n",
    "\n",
    "### 1. input\n",
    "\n",
    "32x32의 크기로 이는 원본데이터의 크기인 28x28보다 살짝 큰데 그 이유는 항상 특징이 최대한 receptive field 중앙에 놓여있기 바라기 때문에 그렇다고 한다. 평균값이 0, 표준편차가 1이 되도록 normalize를 하였다. (대략 바탕이 -0.1, 글씨가 1.175 의 값을 갖는다)\n",
    "\n",
    "### 2. C1\n",
    "\n",
    "5x5 커널를 사용하여 6개의 28x28의 feature map을 가지는 convolution layer이다. parameter 수= (5x5 + 1) x 6 = 156\n",
    "\n",
    "### 3. S1\n",
    "2x2 커널을 사용하여 6개의 28x28 feature map을 14x14로 축소시킨다. parameter 수= (1+1) x 6 = 12\n",
    "\n",
    "### 4. C3\n",
    "\n",
    "기본적으로 C1(우리가 흔히 사용하는 Convolution)과 같지만 6개의 채널을 모두 사용하지 않고 subset만을 사용하여 feature map을 생성한다.\n",
    "16개의 feature map에 대해서 사용하는 input channel은 밑의 표에서 확인할 수 있다.\n",
    "\n",
    "<img src='img/Lenet/c3.png' />\n",
    "\n",
    "왜 모든 channel를 사용하지 않는가라는 의문이 생길 수 있다. 논문에서는 그 이유로 2가지를 설명하는데\n",
    "1. 첫째로, connection의 수와 parameter의 수를 줄일 수 있다.\n",
    "2. 망의 대칭성을 무너트릴 수 있다. 이로써 생성되는 각 feature map이 최대한 다른 특징을 갖도록 유도할 수 있다.\n",
    "\n",
    "parameter 수 = (5*5*3+ 1)*6 + (5*5*4+1)*9 + (5*5*6+1)*1 = 1516\n",
    "\n",
    "### 5. S4\n",
    "\n",
    "C3의 feature map의 높이와 너비를 절반으로 줄인다\n",
    "\n",
    "### 6. C5\n",
    "\n",
    "S4에서 나온 feature map의 크기가 5x5이기 때문에 5x5 convolution을 하게 되면 1x1 feature map이 나온다. 따라서 이 연산은 Fully connected layer와 다름이 없다.\n",
    "\n",
    "parameter 수 = (5x5x16 + 1) *120 = 48120\n",
    "\n",
    "### 7. F6\n",
    "\n",
    "86개의 output을 가지는 linear layer\n",
    "\n",
    "parameter 수 = (120 + 1) x 84 = 10164\n",
    "\n",
    "\n",
    "### 8. Output\n",
    "\n",
    "Output Layer는 class 수만큼의 Euclidean Radial Basis Function(RBF) unit들로 구성되어 있으며, 이게 뭔가하니 parameter vector와 input vector의 거리를 반환하는 layer이다.\n",
    "\n",
    "$$\n",
    "y_i = ||x - w_i||^2\n",
    "$$\n",
    "\n",
    "각 RBF unit은 하나의 class를 대표하는 값으로 수학적으로 보면 unit의 가우시안 분포에서 input의 Negative Log Loss로 해석할 수 있다.\n",
    "각 파라미터 벡터는 -1과 1로 이루어져 있으며 랜덤하게 생성할 수 있지만 논문에서는 다음과 같이 사용하였다.\n",
    "\n",
    "<img src='img/Lenet/rbf_parameter_vectors.png' />\n",
    "\n",
    "이전 layer에서 84라는 애매한 숫자의 output을 반환하는 이유는 12x7 형태의 비트맵으로 parameter vector를 사용하기 싶었기 때문이다. 실제 업무에서 숫자가 아닌 것들을 걸러내는 것을 고려하여 이런 layer를 output에 쓴다고 논문에서 설명되어 있다.\n",
    "\n",
    "###  Squashing Function\n",
    "\n",
    "Scaled tanh 함수를 쓰는데 그 이유는 -1, 1이 이 함수의 active 한 region에서 놀기 바라기 때문이다. f(-1) = -1 , f(1) = 1 이 되도록 설계되었고, 이 지점에서 가장 곡률이 크다(non-linearity를 잘 살릴 수 있다)\n",
    "y = Atanh(Sx) A = 1.7159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅳ. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function은 기본적으로 Maximum Likelihood에 기반한 Mean Squared Error를 사용한다.\n",
    "\n",
    "$$\n",
    "L(W) = \\frac{1}{N}\\sum_{n=1}^N y_{D_i}(Z_i, W)\n",
    "$$\n",
    "\n",
    "하지만 단순히 이 Loss function을 사용했을 때 문제점이 2가지가 있는데,\n",
    "1. rbf의 parameter vector가 동일해지는 방향으로 훈련이 되어 F6의 output또한 constant가 되고 결론적으로 loss값은 0에 가까워지지만 올바른 예측을 하지 않는다. 따라서 parameter vector는 고정시킨다.\n",
    "2. 경쟁을 하지 않는다. 그래서 식은 이건데 이게 보면 cross entropy 식이랑 동일하다. 당시 이런 용어를 안썻나?? 어쨋든 그렇다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16x16 centered in 28x28 -1~1 1channel grayscaled image\n",
    "output = 10 sized vector +1 for correct and -1 for others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Lambda, Pad, Resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    Pad((2,2)),\n",
    "    ToTensor(),\n",
    "    Lambda(lambda tensor: 2 * tensor - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='~/git/cnn/data/', train=True, transform=transform)\n",
    "test_dataset = MNIST(root='~/git/cnn/data/', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = train_dataset[0][0].squeeze(0)\n",
    "sample_img.shape\n",
    "plt.imshow(sample_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PartialConv2d, self).__init__()\n",
    "        self.subsets = [[0,1,2], [1,2,3], [2,3,4], [3,4,5], [0,4,5], [0,1,5],\n",
    "                        [0,1,2,3], [1,2,3,4], [2,3,4,5], [0,3,4,5], [0,1,4,5],\n",
    "                        [0,1,2,5], [0,1,3,4], [1,2,4,5], [0,2,3,5], [0,1,2,3,4,5]]\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(len(subset), 1, 5) for subset in self.subsets])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feature_maps = []\n",
    "        for conv, subset in zip(self.convs, self.subsets):\n",
    "            inp = x[:, subset]\n",
    "            feature_maps.append(conv(inp))\n",
    "        return torch.cat(feature_maps, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel, kernel_size):\n",
    "        super(AvgPool, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size)\n",
    "        self.weight = torch.nn.Parameter(torch.ones(input_channel, 1, 1, dtype=torch.float32))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(input_channel, 1, 1, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * self.avgpool(x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(RBF, self).__init__()\n",
    "        self.parameter_vectors = torch.nn.Parameter(\n",
    "            torch.FloatTensor(np.random.choice([-1, 1], p=[0.65,0.35], size=(out_features, in_features))),\n",
    "            requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cdist(x, self.parameter_vectors)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1.7159 * self.tanh(0.66666*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFn:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def __call__(self, pred, label):\n",
    "        pred = torch.cat([-pred, torch.full((len(pred), 1), -5.0, device=device)], dim=1)\n",
    "        return self.cel(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Lenet1, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            Tanh(),\n",
    "            AvgPool(6, 2),\n",
    "            Tanh(),\n",
    "            PartialConv2d(),\n",
    "            Tanh(),\n",
    "            AvgPool(16, 2),\n",
    "            Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(5*5*16, 120),\n",
    "            Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            Tanh(),\n",
    "            RBF(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60840"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 20\n",
    "\n",
    "lenet1 = Lenet1().to(device)\n",
    "optimizer = torch.optim.SGD(lenet1.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[4,8,12], gamma=0.2)\n",
    "loss_fn = LossFn()\n",
    "\n",
    "total_parameters = 0\n",
    "for i, param in lenet1.named_parameters():\n",
    "    total_parameters += param.numel()\n",
    "    \n",
    "total_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "tensor(3.0532, device='cuda:0')\n",
      "  Train metrics : loss:8.8215, acc:0.9318\n",
      "  Test  metrics : loss:2.9558, acc:0.9772\n",
      "\n",
      "epoch:2\n",
      "tensor(1.7579, device='cuda:0')\n",
      "  Train metrics : loss:2.7306, acc:0.9787\n",
      "  Test  metrics : loss:2.1872, acc:0.9824\n",
      "\n",
      "epoch:3\n",
      "tensor(1.9034, device='cuda:0')\n",
      "  Train metrics : loss:2.0712, acc:0.9840\n",
      "  Test  metrics : loss:1.7218, acc:0.9850\n",
      "\n",
      "epoch:4\n",
      "tensor(1.9981, device='cuda:0')\n",
      "  Train metrics : loss:1.6964, acc:0.9872\n",
      "  Test  metrics : loss:1.8120, acc:0.9850\n",
      "\n",
      "epoch:5\n",
      "tensor(1.2549, device='cuda:0')\n",
      "  Train metrics : loss:1.1627, acc:0.9913\n",
      "  Test  metrics : loss:1.3041, acc:0.9889\n",
      "\n",
      "epoch:6\n",
      "tensor(1.2777, device='cuda:0')\n",
      "  Train metrics : loss:1.0782, acc:0.9922\n",
      "  Test  metrics : loss:1.2978, acc:0.9884\n",
      "\n",
      "epoch:7\n",
      "tensor(1.2703, device='cuda:0')\n",
      "  Train metrics : loss:1.0369, acc:0.9925\n",
      "  Test  metrics : loss:1.2892, acc:0.9887\n",
      "\n",
      "epoch:8\n",
      "tensor(1.2922, device='cuda:0')\n",
      "  Train metrics : loss:1.0062, acc:0.9929\n",
      "  Test  metrics : loss:1.2646, acc:0.9883\n",
      "\n",
      "epoch:9\n",
      "tensor(1.1878, device='cuda:0')\n",
      "  Train metrics : loss:0.9095, acc:0.9937\n",
      "  Test  metrics : loss:1.2354, acc:0.9889\n",
      "\n",
      "epoch:10\n",
      "tensor(1.2314, device='cuda:0')\n",
      "  Train metrics : loss:0.8969, acc:0.9938\n",
      "  Test  metrics : loss:1.2299, acc:0.9884\n",
      "\n",
      "epoch:11\n",
      "tensor(1.2296, device='cuda:0')\n",
      "  Train metrics : loss:0.8927, acc:0.9938\n",
      "  Test  metrics : loss:1.2135, acc:0.9888\n",
      "\n",
      "epoch:12\n",
      "tensor(1.2135, device='cuda:0')\n",
      "  Train metrics : loss:0.8851, acc:0.9937\n",
      "  Test  metrics : loss:1.2068, acc:0.9896\n",
      "\n",
      "epoch:13\n",
      "tensor(1.2181, device='cuda:0')\n",
      "  Train metrics : loss:0.8648, acc:0.9939\n",
      "  Test  metrics : loss:1.2071, acc:0.9892\n",
      "\n",
      "epoch:14\n",
      "tensor(1.2171, device='cuda:0')\n",
      "  Train metrics : loss:0.8630, acc:0.9941\n",
      "  Test  metrics : loss:1.2044, acc:0.9893\n",
      "\n",
      "epoch:15\n",
      "tensor(1.2163, device='cuda:0')\n",
      "  Train metrics : loss:0.8606, acc:0.9941\n",
      "  Test  metrics : loss:1.2008, acc:0.9892\n",
      "\n",
      "epoch:16\n",
      "tensor(1.2259, device='cuda:0')\n",
      "  Train metrics : loss:0.8603, acc:0.9940\n",
      "  Test  metrics : loss:1.2052, acc:0.9890\n",
      "\n",
      "epoch:17\n",
      "tensor(1.2244, device='cuda:0')\n",
      "  Train metrics : loss:0.8607, acc:0.9940\n",
      "  Test  metrics : loss:1.2019, acc:0.9891\n",
      "\n",
      "epoch:18\n",
      "tensor(1.2245, device='cuda:0')\n",
      "  Train metrics : loss:0.8568, acc:0.9942\n",
      "  Test  metrics : loss:1.2034, acc:0.9888\n",
      "\n",
      "epoch:19\n",
      "tensor(1.2191, device='cuda:0')\n",
      "  Train metrics : loss:0.8563, acc:0.9940\n",
      "  Test  metrics : loss:1.2005, acc:0.9889\n",
      "\n",
      "epoch:20\n",
      "tensor(1.2206, device='cuda:0')\n",
      "  Train metrics : loss:0.8550, acc:0.9941\n",
      "  Test  metrics : loss:1.1984, acc:0.9892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    loss_tot, acc = 0, 0\n",
    "    for X, label in train_dataloader:\n",
    "        X, label = X.to(device), label.to(device)\n",
    "        pred_y = lenet1(X)\n",
    "        loss = loss_fn(pred_y, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tot += loss.item()\n",
    "        pred_label = (pred_y).argmin(dim=1)\n",
    "        acc += (pred_label == label).sum().item() / len(label)\n",
    "    scheduler.step()\n",
    "    loss_test, acc_test =  0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, label in test_dataloader:\n",
    "            X, label = X.to(device), label.to(device)\n",
    "            pred_y = lenet1(X)\n",
    "            loss = loss_fn(pred_y, label)\n",
    "            loss_test += loss.item()\n",
    "            min_val, pred_label = (pred_y).min(dim=1)\n",
    "            acc_test += (pred_label == label).sum().item() / len(label)\n",
    "    print(f'epoch:{epoch}')\n",
    "    print(min_val.mean())\n",
    "    print(f'  Train metrics : loss:{loss_tot/len(train_dataloader):.4f}, acc:{acc/len(train_dataloader):.4f}')\n",
    "    print(f'  Test  metrics : loss:{loss_test/len(test_dataloader):.4f}, acc:{acc_test/len(test_dataloader):.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in lenet1.net[13].parameter_vectors.cpu().detach().numpy():\n",
    "    plt.imshow(x.reshape(12,7))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
